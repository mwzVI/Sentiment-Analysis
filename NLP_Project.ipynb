{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b50610",
   "metadata": {},
   "source": [
    "<H1>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NLP (Natural Language Processing) PROJECT</H1>\n",
    "\n",
    "<H2>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IMDB SENTIMENT ANALYSIS USING NLP </H2>\n",
    "\n",
    "\n",
    "You can download the dataset from the Kaggle link below: https://www.kaggle.com/ymanojkumar023/kumarmanoj-bag-of-words-meets-bags-of-popcorn \n",
    "\n",
    "Please download only: labeledTrainData.tsv file..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our dataset here..\n",
    "df = pd.read_csv('labeledTrainData.tsv',  delimiter=\"\\t\", quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the head of the dataset:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e767dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since stopwords aren't used in NLP we have to clear stopwords. For this purpose we must download stopword wordset from nltk library..\n",
    "# We do this operation using nltk module..\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748918b",
   "metadata": {},
   "source": [
    "## * * * * Data Cleaning Operations * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb420a",
   "metadata": {},
   "source": [
    "### First, we will delete HTML tags from review sentences using the BeautifulSoup module.\n",
    "\n",
    "To explain how these processes are done, first let's choose a single review and see how it is done for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52044274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = df.review[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d67c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear HTML tags..\n",
    "sample = BeautifulSoup(sample).get_text()\n",
    "sample  # After the HTML tags are cleared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean it from punctuation and numbers - using regex..\n",
    "sample = re.sub(\"[^a-zA-Z]\",' ',sample)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64917f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert it to lowercase, since our machine learning algorithms think capitalized letters with different words\n",
    "# and this may result mistakes..\n",
    "sample = sample.lower()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords (stopwords are the words like the, is, are not to be used by AI. These are grammar words..)\n",
    "# First we split the words with split and convert them to a list. Our goal is to remove stopwords..\n",
    "sample = sample.split()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample without stopwords\n",
    "swords = set(stopwords.words(\"english\"))                      # conversion into set for fast searching\n",
    "sample = [w for w in sample if w not in swords]               \n",
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57027ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b90371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After describing the cleanup process, we now batch clean the reviews in our entire dataframe in a loop\n",
    "# for this purpose we first create a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(review):\n",
    "    # without HTML tags\n",
    "    review = BeautifulSoup(review).get_text()\n",
    "    # without punctuation and numbers\n",
    "    review = re.sub(\"[^a-zA-Z]\",' ',review)\n",
    "    # lowercase and splitting to eliminate stopwords\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    # without stopwords\n",
    "    swords = set(stopwords.words(\"english\"))                      \n",
    "    review = [w for w in review if w not in swords]               \n",
    "    # we join splitted paragraphs with join before return..\n",
    "    return(\" \".join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594912eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean our training data with the help of the above function:\n",
    "# We can see the status of the review process by printing a line after every 1000 reviews.\n",
    "\n",
    "train_x_tum = []\n",
    "for r in range(len(df[\"review\"])):        \n",
    "    if (r+1)%1000 == 0:        \n",
    "        print(\"No of reviews processed =\", r+1)\n",
    "    train_x_tum.append(process(df[\"review\"][r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2a1c0",
   "metadata": {},
   "source": [
    "### Train, test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d340b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to split our data as train and test..\n",
    "x = train_x_tum\n",
    "y = np.array(df[\"sentiment\"])\n",
    "\n",
    "# train test split\n",
    "train_x, test_x, y_train, y_test = train_test_split(x,y, test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590370e4",
   "metadata": {},
   "source": [
    "### We are building our Bag of Words !\n",
    "\n",
    "We have cleaned our data, but for the artificial intelligence to work, it is necessary to convert this text-based data into numbers and a matrix called bag of words. Here we use the CountVectorizer tool in sklearn for this purpose:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d80813",
   "metadata": {},
   "source": [
    "<IMG src=\"bag.jpg\" width=\"900\" height=\"900\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the countvectorizer function in sklearn, we create a bag of words with a maximum of 5000 words...\n",
    "vectorizer = CountVectorizer( max_features = 5000 )\n",
    "\n",
    "# We convert our train data to feature vector matrix\n",
    "train_x = vectorizer.fit_transform(train_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9925e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are converting it to array because it wants array for fit operation..\n",
    "train_x = train_x.toarray()\n",
    "train_y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddaafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658fe33",
   "metadata": {},
   "source": [
    "### We build and fit a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
    "model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b440b1a5",
   "metadata": {},
   "source": [
    "### Now it's time for our test data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert our test data to feature vector matrix\n",
    "test_xx = vectorizer.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbfd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1959911",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xx = test_xx.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0be3d",
   "metadata": {},
   "source": [
    "#### Now let's predict.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e913a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_xx)\n",
    "acc = roc_auc_score(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: % \", acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd665d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a750489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66db775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c6d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31354a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d4210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13722a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035d62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c02ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cc7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362f38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62913061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2de634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb8202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37391e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe125b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
